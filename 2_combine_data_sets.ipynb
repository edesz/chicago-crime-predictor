{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Combine datasets](#combine-datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.visualization_helpers\n",
    "from src.visualization_helpers import plot_horiz_bar\n",
    "\n",
    "%aimport src.preprocess_helpers\n",
    "from src.preprocess_helpers import (\n",
    "    append_clean_data,\n",
    "    append_demographic_data,\n",
    "    drop_non_zero_rows,\n",
    "    explode,\n",
    "    load_merge_slice_data,\n",
    "    merge_with_weather_data,\n",
    "    point_inside_polygon,\n",
    "    write_data_to_csv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0. [About](#about)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Load crime listings data](#load-crime-listings-data)\n",
    "3. [Get demographic data](#get-demographic-data)\n",
    "4. [Load weather data](#load-weather-data)\n",
    "5. [Extract neighborhood name from joined data](#extract-neighborhood-name-from-joined-data)\n",
    "6. [Summarize joined data](#summarize-joined-data)\n",
    "7. [Export joined data](#export-joined-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will combine crime listings, weather, demographics and GIS data into a single file at `data/all_joined__<YYYYmmdd_HHMMSS>.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define below the variables that are to be used throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "data_dir_path = str(Path().cwd() / \"data\")\n",
    "weather_data_file_path = str(Path(data_dir_path) / \"raw\" / \"1914019.csv\")\n",
    "joined_data_path = str(\n",
    "    Path(data_dir_path)\n",
    "    / \"processed\"\n",
    "    / f\"all_joined__{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    ")\n",
    "years_wanted = [2018, 2019]\n",
    "months_wanted = [1, 2, 3]\n",
    "cols_to_drop = [\"ID\"]\n",
    "\n",
    "dtypes_dict = {\n",
    "    \"id\": int,\n",
    "    \"case_number\": str,\n",
    "    \"date\": str,\n",
    "    \"block\": str,\n",
    "    \"iucr\": str,\n",
    "    \"primary_type\": str,\n",
    "    \"description\": str,\n",
    "    \"location_description\": str,\n",
    "    \"arrest\": bool,\n",
    "    \"domestic\": bool,\n",
    "    \"beat\": int,\n",
    "    \"district\": int,\n",
    "    \"ward\": float,\n",
    "    \"community_area\": float,\n",
    "    \"fbi_code\": str,\n",
    "    \"X Coordinate\": float,\n",
    "    \"Y Coordinate\": float,\n",
    "    \"year\": int,\n",
    "    \"updated_on\": str,\n",
    "    \"latitude\": float,\n",
    "    \"longitude\": float,\n",
    "    \"location\": str,\n",
    "    \"Historical Wards 2003-2015\": float,\n",
    "    \"Zip Codes\": float,\n",
    "    \"Community Areas\": float,\n",
    "    \"Census Tracts\": float,\n",
    "    \"Wards\": float,\n",
    "    \"Boundaries - ZIP Codes\": float,\n",
    "    \"Police Districts\": float,\n",
    "    \"Police Beats\": float,\n",
    "}\n",
    "unwanted_cols = [\"case_number\", \"date\", \"x_coordinate\", \"y_coordinate\", \"updated_on\"]\n",
    "wanted_weather_cols = [\n",
    "    \"AWND\",\n",
    "    \"PRCP\",\n",
    "    \"SNOW\",\n",
    "    \"SNWD\",\n",
    "    \"TAVG\",\n",
    "    \"TMAX\",\n",
    "    \"TMIN\",\n",
    "    \"WDF2\",\n",
    "    \"WDF5\",\n",
    "    \"WSF2\",\n",
    "    \"WSF5\",\n",
    "    \"WT01\",\n",
    "    \"date_yymmdd\",\n",
    "]\n",
    "\n",
    "weather_data_date_col = \"DATE\"\n",
    "merge_data_on = \"date_yymmdd\"\n",
    "merge_weather_data_on = \"date_yymmdd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path, weather_data_file_path, joined_data_path = [\n",
    "    Path(data_dir_path),\n",
    "    Path(weather_data_file_path),\n",
    "    Path(joined_data_path),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-crime-listings-data\"></a>\n",
    "\n",
    "## 1. [Load crime listings data](#load-crime-listings-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by loading and cleaning the crime listings data from the [Chicago open data portal](https://data.cityofchicago.org/browse?limitTo=datasets) for the years [2018](https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy) and [2019](https://data.cityofchicago.org/Public-Safety/Crimes-2019/w98m-zvie)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by performing the following\n",
    "- read all data files corresponding to crime listings data into a single `DataFrame`\n",
    "- change all column names to lower case\n",
    "- replace spaces in column names by underscores\n",
    "- drop the `ID` column\n",
    "\n",
    "Next, we'll filter by year(s) and month(s) that we want to include in our analysis - 2018-2019 and January-March respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "df = load_merge_slice_data(\n",
    "    dtypes_dict=dtypes_dict,\n",
    "    file_paths=glob(str(data_dir_path / \"raw\" / \"Crime*.csv\")),\n",
    "    years_wanted=years_wanted,\n",
    "    months_wanted=months_wanted,\n",
    "    cols_to_drop=cols_to_drop,\n",
    ")\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape[0])\n",
    "display(df.head())\n",
    "display(df.dtypes.to_frame())\n",
    "print(\n",
    "    f\"Number of rows of data left after dropping records with unwanted years and months: {df.shape[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll print the number of rows of missing values for all columns of the crime listings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While several features have missing data, these include some of the unwanted features we specified in the user input section earlier. Those unwanted columns should be dropped before dropping rows with missing values in any feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a helper fucntion to geneate features based on `datetime` attributes, filter out unwanted columns and then drop rows with any missing values. This will include two boolean features\n",
    "- `is_weekend` a boolean to check if the date falls on a weekday or weekend\n",
    "- `is_dark` a boolean to check if the hour of the day falls during a manually specified time of day when the sky is dark outside or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = append_clean_data(df=df, unwanted_cols=unwanted_cols)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll examine the number of rows of missing data in all columns of the crime listings `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.isna().sum(), columns=[\"number_of_missing_values\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get-demographic-data\"></a>\n",
    "\n",
    "## 3. [Get demographic data](#get-demographic-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll use the []() package to extract demographic data about the area immediately surrounding the locations where crimes were committed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this, we'll start by creating several helper functions to extract a single attribute of demographics data, based on its latitude and longitude\n",
    "- if demographic data is not available for a specific point, we'll return a zero for that attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use a [vectorized version of each function](https://stackoverflow.com/a/52674448/4057186) to retrieve their corresponding demographic attribute based on the latitude and longitude, and append this value to the `DataFrame` of crime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "df_execution_times, d = append_demographic_data(df)\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll show the execution time required to retrieve each demographic attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_execution_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll check how many rows of the `DataFrame` are missing demographic data\n",
    "- since the helper functions earlier returned 0 if demographic data was missing, we'll filter for zeros as our proxy for missing demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_demographic_data = pd.DataFrame(\n",
    "    df[list(d.keys())][df[list(d.keys())] == 0].count(), columns=[\"num_rows_of_zeros\"]\n",
    ")\n",
    "df_no_demographic_data[\"num_rows\"] = df.shape[0]\n",
    "df_no_demographic_data[\"num_rows_of_non_zeros\"] = (\n",
    "    df_no_demographic_data[\"num_rows\"] - df_no_demographic_data[\"num_rows_of_zeros\"]\n",
    ")\n",
    "df_no_demographic_data[\"percent_of_zeros\"] = (\n",
    "    df_no_demographic_data[\"num_rows_of_zeros\"] / df_no_demographic_data[\"num_rows\"]\n",
    ") * 100\n",
    "df_no_demographic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll drop all rows of the joined data where demographic data are recorded as zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_non_zero_rows(df, col_rows_to_drop=\"total_population\")\n",
    "print(\n",
    "    f\"Number of rows of data left after dropping records with no demographic data: {df.shape[0]}\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-weather-data\"></a>\n",
    "\n",
    "## 4. [Load weather data](#load-weather-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll load weather data for the city of Chicago (taken from the [Chicago O'Hare International airport station](https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00094846/detail)) to the crime listings data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the NOAA weather data and then append a column that gives the `date` as a `datetime`. Next, we'll merge weather and crime data on the common column of `date_yymmdd`, which is the `datetime` version of the raw `date` from each `DataFrame`. As we're only retaining a subset of features of the weather data, we'll only include those when merging weather and crime `DataFrame`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merge_with_weather_data(\n",
    "    df_data=df,\n",
    "    weather_data_file_path=Path(weather_data_file_path),\n",
    "    weather_data_date_col=weather_data_date_col,\n",
    "    wanted_weather_cols=wanted_weather_cols,\n",
    "    merge_data_on=merge_data_on,\n",
    "    merge_weather_data_on=merge_weather_data_on,\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"extract-neighborhood-name-from-joined-data\"></a>\n",
    "\n",
    "## 5. [Extract neighborhood name from joined data](#extract-neighborhood-name-from-joined-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll append a `neighborhood` column to the joined `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by reading in the [Chicago open data boundary file for Neighborhoods](https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-Neighborhoods/bbvz-uum9) using [`geopandas`](http://geopandas.org/).\n",
    "\n",
    "Next, we'll explode all [`POLYGON`](https://macwright.org/2015/03/23/geojson-second-bite.html#polygons) objects (tuples of latitude-longitude co-ordinates) in the `geometry` column of the [`geopandas` `geoDataFrame`](https://gist.github.com/mhweber/cf36bb4e09df9deee5eb54dc6be74d26#gistcomment-2353309) and then store them in a list of latitude-longitude co-ordinates. We're doing this so that we can later [check if a single latitude-longitude tuple is in any of the nested (exploded) list of latitude-longitude tuples](https://medium.com/dataexplorations/working-with-open-data-shape-files-using-geopandas-how-to-match-up-your-data-with-the-areas-9377471e49f2), which we will use to determine the name of the neighborhood where a crime was committed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "gdf_out = explode(\n",
    "    path_to_file=glob(str(data_dir_path / \"raw\" / \"Neighborhoods\" / \"*.shp\"))[0]\n",
    ")\n",
    "display(gdf_out.head(2))\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll extract the `neighborhood` for each row of the data and append this as a new column to the `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbourhood(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Get neighborhood from lat, long and neighbourhood boudnary lat-long\n",
    "    SOURCE: https://medium.com/dataexplorations/\n",
    "            working-with-open-data-shape-files-using-geopandas-how-to-match-\n",
    "            up-your-data-with-the-areas-9377471e49f2\n",
    "    \"\"\"\n",
    "    for ix, area in gdf_out.iterrows():\n",
    "        is_in_area = False\n",
    "        if row[\"latitude\"] and row[\"longitude\"]:\n",
    "            is_in_area = point_inside_polygon(\n",
    "                row[\"latitude\"], row[\"longitude\"], area[\"geomlist\"]\n",
    "            )\n",
    "            if is_in_area:\n",
    "                return area[\"pri_neigh_x\"]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "df[\"neighbourhood\"] = df.apply(lambda row: get_neighbourhood(row), axis=1)\n",
    "display(df.head())\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summarize-joined-data\"></a>\n",
    "\n",
    "## 6. [Summarize joined data](#summarize-joined-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a `DataFrame` showing all the features that will be written to a `*.csv` file\n",
    "- `block` will not be written to an output file since this feature is comprised of very low frequency values\n",
    "- `WT01` will not be written since it is nearly unchanged for all selected rows (winter season months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = {\n",
    "    # \"block\": [str, \"Name of block where crime occurred\", \"CATEGORICAL\"],\n",
    "    \"arrest\": [bool, \"Arrest made?\", \"CATEGORICAL\"],\n",
    "    \"domestic\": [bool, \"Domestic Violence?\", \"CATEGORICAL\"],\n",
    "    \"beat\": [int, \"Smallest police grographic area\", \"CATEGORICAL\"],\n",
    "    \"district\": [int, \"Police District\", \"CATEGORICAL\"],\n",
    "    \"ward\": [float, \"City Council District\", \"CATEGORICAL\"],\n",
    "    \"community_area\": [float, \"Community identifier\", \"CATEGORICAL\"],\n",
    "    \"fbi_code\": [str, \"FBI Crime Classification\", \"CATEGORICAL\"],\n",
    "    \"day_name\": [str, \"weekday\", \"CATEGORICAL\"],\n",
    "    \"month\": [int, \"month\", \"CATEGORICAL\"],\n",
    "    \"day\": [int, \"day of month\", \"CATEGORICAL\"],\n",
    "    \"hour\": [int, \"hour of day\", \"CATEGORICAL\"],\n",
    "    \"weekofyear\": [int, \"week of year\", \"CATEGORICAL\"],\n",
    "    \"total_population\": [int, \"Surrounding total population\", \"NUMERIC\"],\n",
    "    \"housing_units\": [int, \"Number of surrounding housing units\", \"NUMERIC\"],\n",
    "    \"median_household_value\": [int, \"Median surrounding household value\", \"NUMERIC\"],\n",
    "    \"median_household_income\": [int, \"Median surrounding household income\", \"NUMERIC\"],\n",
    "    \"occupied_housing_values\": [\n",
    "        int,\n",
    "        \"Number of surrounding occupied housing units\",\n",
    "        \"NUMERIC\",\n",
    "    ],\n",
    "    \"TAVG\": [float, \"Average temperature\", \"NUMERIC\"],\n",
    "    \"SNOW\": [float, \"Quantity of snowfall\", \"NUMERIC\"],\n",
    "    # \"WT01\": [float, \"Fog, ice fog, or freezing fog (may include heavy fog)\", \"CATEGORICAL\"],\n",
    "    \"neighbourhood\": [str, \"Neighborhood containing event\", \"CATEGORICAL\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = pd.DataFrame.from_dict(\n",
    "    f, orient=\"index\", columns=[\"dtype\", \"Description\", \"Feature_Type\"]\n",
    ")\n",
    "df_cols = df_cols.merge(\n",
    "    pd.DataFrame(df[list(f.keys())].nunique(), columns=[\"nunique\"]),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "df_cols.loc[df_cols[\"Feature_Type\"] == \"NUMERIC\", \"nunique\"] = np.nan\n",
    "df_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll show a bar chart of the `primary_type` column (i.e. the type of crime) in order ot visualize class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horiz_bar(\n",
    "    df=df,\n",
    "    col_name=\"primary_type\",\n",
    "    ptitle=f\"Class Balance for {df.shape[0]:,d} instances\",\n",
    "    fig_size=(10, 10),\n",
    "    savefig=Path().cwd() / \"reports\" / \"figures\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"export-joined-data\"></a>\n",
    "\n",
    "## 7. [Export joined data](#export-joined-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data_to_csv(df=df, joined_data_path=joined_data_path, write_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
